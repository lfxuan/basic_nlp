# LoRA 知识

## 优缺点

优点：

* 节约显存（减少显存占用的主要原因是**训练参数变小**了（比如只对 qkv 层做 LoRA））
* 训练快
* 效果损失较小（相对于全参数微调）
* 推理的时候不增加耗时，可以做一个插入式组件使用

缺点：

* 效果的损失

## LoRA 的实现原理

核心原理非常的简单，任意一个矩阵$\\W_0$，都可以对它进行低秩分解，把一个很大的矩阵拆分成两个小矩矩阵$(A,B)$，在训练的过程中不去改变 $\\W_0$参数，而是去改变$AB$，具体可以表示为
$$
W_{new} = W_0 + AB
$$
最终在训练计算的时候是
$$
h = W_0x + ABx = (W_0 + AB)x
$$
但是一般来说，$AB$会进行一定的缩放，使用 $\frac{\alpha}{\gamma}$作为缩放因子，所以最终会写成
$$
h = (W_0 +\frac{\alpha}{\gamma}AB)x
$$

$$
W_0 \in \mathbb{R}^{n \times m}, A \in \mathbb{R}^{n \times r}, B \in \mathbb{R}^{r \times m}
$$

其中$r \ll n \ and \ r \ll m$，$r$甚至可以设置成 1。

- 为什么说只优化 $AB$ 两个矩阵就可以了呢？这里面的假设是什么？
- $W$不是满秩的，里面有大量参数是冗余的，那么其实可以用更接近满秩的矩阵 $AB$  代替。

> 矩阵都可以表示为若干个线性无关向量，最大的线性无关向量个数就是秩

## 参考

* [LoRA 原理和 PyTorch 代码实现](https://yuanchaofa.com/hands-on-code/hands-on-lora.html)